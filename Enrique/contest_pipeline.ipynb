{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The purpose of this notebook is to assemble a pipeline for preparing the data, transforming it and then training a model to predict house prices. \n",
    "\n",
    "On this project, we will explore the increasingly popular [Ames dataset](https://www.notion.so/Diccionario-de-Datos-y-hints-8f8613b67b4140f1940f67463c4a0ced#bc3273399294410083987b036aef2356). Our goal is to predict the sale price of a house given the rest of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:32.293706Z",
     "start_time": "2020-10-29T14:28:32.284825Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score \n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:32.694130Z",
     "start_time": "2020-10-29T14:28:32.677421Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing. \n",
    "\n",
    "A couple of things had to be done when preparing the data. As explained in the link provided in the first cell, it is best to remove houses with a \"Gr Liv Area\" above 4,000 and houses sold in abnormal conditions. Other than that, the rest of this process consisted of iteratively finding which columns to keep and which to drop.\n",
    "\n",
    "There are more than 70 columns, so we won't explain in full detail why we chose some and left others out. But the main idea was to look for a parsimonious, coherent and logical model to predict house prices. We were perhaps a bit ruthless on dropping most categorical variables for their potential to increase dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:35.381060Z",
     "start_time": "2020-10-29T14:28:35.377061Z"
    }
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('casas_entrena.csv')\n",
    "#data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:35.932453Z",
     "start_time": "2020-10-29T14:28:35.923861Z"
    }
   },
   "outputs": [],
   "source": [
    "#we want to clean the column names \n",
    "def clean_column(col):\n",
    "    return col.lower().replace('/','_').replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:36.638294Z",
     "start_time": "2020-10-29T14:28:36.622884Z"
    }
   },
   "outputs": [],
   "source": [
    "def replacing_nans(data):\n",
    "    \"\"\"\n",
    "    Function to fill the columns with Nan Values where it is convenient to keep them.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pandas dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data: pandas dataframe.\n",
    "    \"\"\"\n",
    "    data[\"pool_qc\"] = data[\"pool_qc\"].fillna(\"None\") #Pool brings useful information\n",
    "    data[\"pool_qc\"] = np.where(data[\"pool_qc\"] == \"None\", 0, 1)\n",
    "    data[\"fireplace_qu\"] = data[\"fireplace_qu\"].fillna(\"None\")\n",
    "    #Garage information might be useful\n",
    "    for col in ('garage_type', 'garage_finish', 'garage_qual', 'garage_cond'):\n",
    "        data[col] = data[col].fillna('None')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:37.378623Z",
     "start_time": "2020-10-29T14:28:37.364915Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_preprocessing(path):\n",
    "    \"\"\"\n",
    "    Una función básica para preprocesar los datos de entrenamiento. \n",
    "    Parameters:\n",
    "    -------\n",
    "    path: str\n",
    "          path en tu compu donde está el dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data: pandas dataframe\n",
    "          Un dataframe listo para el pipeline de sklearn. \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[data['Sale Condition'] == \"Normal\"]\n",
    "    data = data.drop(columns = COLS_TO_DROP)\n",
    "    data = data[data['Gr Liv Area'] < 4_000]\n",
    "    data.rename(columns={col: clean_column(col) for col in data.columns.values}, \n",
    "                 inplace=True)\n",
    "    data = replacing_nans(data)\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:38.228831Z",
     "start_time": "2020-10-29T14:28:38.225783Z"
    }
   },
   "outputs": [],
   "source": [
    "#this is what it looks like right now\n",
    "#data = basic_preprocessing('casas_entrena.csv')\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:41.302241Z",
     "start_time": "2020-10-29T14:28:41.287955Z"
    }
   },
   "outputs": [],
   "source": [
    "#data.shape #this is slightly more parsimonious (compared to 80+ columns!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data. \n",
    "\n",
    "We have done a basic preprocessing of the data just for it to be a little bit cleaner and prepared for a ML model. Now we will focus on the transformation pipeline to be implemented later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder. \n",
    "\n",
    "Some categorical features —kitchen quality, for example— are arrenged in an ordering such that it may be useful to transform them with Sklearn's LabelEncoder() so that we can further manipulate them and combine them with other features. \n",
    "\n",
    "This has the added benefit of reducing the number of variables in the final model because we are not one-hot-encoding them into n or n-1 new variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:43.414346Z",
     "start_time": "2020-10-29T14:28:43.400245Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_variables(data, cols):\n",
    "    \"\"\"\n",
    "    Function to transform numerical features into a numerical ordering. \n",
    "    \n",
    "    Parameters: \n",
    "    -----------\n",
    "    data: pandas dataframe\n",
    "    cols: list\n",
    "          list of cols that you want to transform with the label encoder. \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    data: pandas dataframe\n",
    "          A cleanear, happier dataframe :) \n",
    "    \n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        le = LabelEncoder() \n",
    "        data[col] = le.fit_transform(list(data[col].values))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fun part. Now we get to add new variables and interactions between some of them. For example, it makes sense to add a variable that shows the interaction between total square feet and the overall quality of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:45.299599Z",
     "start_time": "2020-10-29T14:28:45.277754Z"
    }
   },
   "outputs": [],
   "source": [
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, house_condition = True): \n",
    "        self.house_condition = house_condition\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        qual_squared = X[:, idx[\"overall_qual\"]] ** 2 #quality squared\n",
    "        qual_m2 = X[:, idx[\"overall_qual\"]] * X[:, idx[\"lot_area\"]] #quality * sq. feet\n",
    "       # garage_int = X[:, idx[\"garage_area\"]] * X[:, idx[\"garage_qual\"]]\n",
    "        total_sf = X[:, idx['total_bsmt_sf']] + X[:, idx['1st_flr_sf']] +\\\n",
    "                   X[:, idx['2nd_flr_sf']]\n",
    "        qual_sf_total = X[:, idx[\"overall_qual\"]] * total_sf\n",
    "        #kitchen_total = X[:, idx[\"kitech_qual\"]]\n",
    "        return np.c_[X, qual_squared, qual_m2, total_sf, \n",
    "                    qual_sf_total]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn pipeline. \n",
    "\n",
    "Now it is time to deploy a transformation pipeline to streamline and automate most of the recurring operations.\n",
    "\n",
    "We learned this from Geron's book \"Hands-on Machine Learning with Scikit-learn, Keras, and TensorFlow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:46.453639Z",
     "start_time": "2020-10-29T14:28:46.447392Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:47.157552Z",
     "start_time": "2020-10-29T14:28:47.144224Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "    (\"numeric\", numeric_pipeline, num_attribs),\n",
    "    (\"categorical\", OneHotEncoder(handle_unknown='ignore'), cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing up the preprocessing. \n",
    "\n",
    "After the basic preprocessing (for cleaning up the data), we then transformed the numerical and categorical features into a simpler model. We can put this in one function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:28:48.894563Z",
     "start_time": "2020-10-29T14:28:48.884108Z"
    }
   },
   "outputs": [],
   "source": [
    "def preparing_data(data, cols_to_encode):\n",
    "    \"\"\"\n",
    "    Function designed to transformed a slightly preprocessed dataset into numpy arrays\n",
    "    fit for the model. This will call the sklearn pipeline we defined above. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pandas dataframe\n",
    "          You can plug in the result of basic_preprocessing('casas_entrena.csv')\n",
    "          \n",
    "    cols_to_encode: list\n",
    "                    List of columns you want to transform with the LabelEncoder()\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    X_train_prepared: numpy.ndarray\n",
    "       Transformed features.\n",
    "    \n",
    "    Y: numpy.ndarray\n",
    "       The labels transformed. \n",
    "    \n",
    "    \"\"\"\n",
    "    X = data.iloc[:,:-1].copy() #first we split each into its own category\n",
    "    Y = data[\"saleprice\"].copy()\n",
    "    Y_log = np.log(Y)\n",
    "    X = encode_variables(X, cols_to_encode)\n",
    "    num_attribs = X.select_dtypes(include = \"number\").columns.values\n",
    "    cat_attribs = X.select_dtypes(include = 'object').columns.values\n",
    "    idx = {col: X.columns.get_loc(col) for col in X.columns}\n",
    "    X_train_prepared = full_pipeline.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    return X_train_prepared, Y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:29:18.984148Z",
     "start_time": "2020-10-29T14:29:18.977869Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_encode = ['exter_qual', 'exter_cond', 'heating_qc', 'central_air',\n",
    "                 'kitchen_qual', 'fireplace_qu', 'garage_qual', 'garage_cond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:29:19.579361Z",
     "start_time": "2020-10-29T14:29:19.573327Z"
    }
   },
   "outputs": [],
   "source": [
    " PATH = 'casas_entrena.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:29:49.652775Z",
     "start_time": "2020-10-29T14:29:49.643513Z"
    }
   },
   "outputs": [],
   "source": [
    "COLS_TO_DROP = ['MS Zoning', \n",
    "                'Lot Frontage',\n",
    "                'Lot Shape',\n",
    "                'Street',\n",
    "                'Land Contour',\n",
    "                'Utilities',\n",
    "                'Lot Config',\n",
    "                'Land Slope',\n",
    "                'Neighborhood',\n",
    "                'Alley', \n",
    "                'Mas Vnr Type', \n",
    "                'Mas Vnr Area',          \n",
    "                'Bsmt Qual', \n",
    "                'Bsmt Cond', \n",
    "                'Bsmt Exposure', \n",
    "                'BsmtFin Type 1',\n",
    "                'BsmtFin Type 2',\n",
    "                'Electrical',\n",
    "                #FirePlace Qu, \n",
    "                #'Garage Type', \n",
    "                'Garage Yr Blt', \n",
    "               # 'Garage Finish', \n",
    "               # 'Garage Qual',\n",
    "               # 'Garage Cond',\n",
    "                #'Pool QC', \n",
    "                'Fence', \n",
    "                'Misc Feature',\n",
    "                'Condition 1', \n",
    "                'Condition 2',\n",
    "                'Exterior 1st', \n",
    "                'Exterior 2nd', \n",
    "                'Heating', \n",
    "                #'Heating QC',\n",
    "               'Roof Style',\n",
    "               'Roof Matl',\n",
    "               'Foundation',\n",
    "               'Functional',\n",
    "               'Fireplaces',\n",
    "               'Paved Drive',  #maybe keep it\n",
    "               'Year Remod/Add',\n",
    "               '3Ssn Porch',\n",
    "               'Pool Area', #we already have a categorical variable for pool\n",
    "               'Mo Sold',\n",
    "               'Misc Val',\n",
    "               'Open Porch SF',\n",
    "               'BsmtFin SF 2',\n",
    "               'Wood Deck SF',\n",
    "               'Enclosed Porch',\n",
    "               'Screen Porch',\n",
    "               'Sale Condition',\n",
    "               'Sale Type'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:29:53.759864Z",
     "start_time": "2020-10-29T14:29:53.658388Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = preparing_data(basic_preprocessing(PATH), cols_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. \n",
    "\n",
    "Lo que puedo hacer para después, el 30  de octubre o así. \n",
    "\n",
    "- Tirar más columnas (chance fireplace quality nelson y otras así, igual y 60 al final son muchas.\n",
    "- Building type tirarla igual. \n",
    "- Intentar crear automáticamente la gráfica de las predicciones en el entrenamiento. \n",
    "- La meta es llegar a menos de  7% de error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model. \n",
    "\n",
    "We will use linear regression, regularization and cross validation to train, adjust and evaluate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:29:57.456193Z",
     "start_time": "2020-10-29T14:29:57.448418Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_scores(X_train, Y_train, CV):\n",
    "    \"\"\"\n",
    "    Just a simple function to get scores from cross validation. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: Prepared features.\n",
    "    Y_train: Prepared labels. \n",
    "    CV: Number of cross validation folds.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    None\n",
    "    \"\"\"\n",
    "    Reg = Ridge()\n",
    "    scores = cross_val_score(ridge_reg, X_train, Y_train, scoring = \"neg_mean_squared_log_error\", cv = CV)\n",
    "    ridge_reg_scores = np.sqrt(-scores)\n",
    "    print(\"These are your scores:\")\n",
    "    print()\n",
    "    print(ridge_reg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:30:02.478255Z",
     "start_time": "2020-10-29T14:30:02.417551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are your scores:\n",
      "\n",
      "[0.00858856 0.01053167 0.00746376 0.00841631 0.00740729]\n"
     ]
    }
   ],
   "source": [
    "get_initial_scores(X_train, Y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:30:03.979474Z",
     "start_time": "2020-10-29T14:30:03.969484Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'alpha': [.001, .01, .1, 1, 10, 15, 20, 25, 30], \n",
    "     'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:30:04.646554Z",
     "start_time": "2020-10-29T14:30:04.639039Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_searching(X_train, Y_train, CV):\n",
    "    \"\"\"\n",
    "    Simple function to simplify all this process.\n",
    "    Same parameters as above.\n",
    "    \"\"\"\n",
    "    ridge = Ridge()\n",
    "    grid_search = GridSearchCV(ridge, param_grid, cv = CV, scoring = \"neg_mean_squared_log_error\",\n",
    "                          return_train_score = True, n_jobs=-1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    print(\"These are your best parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    #return the best model\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:22:29.319663Z",
     "start_time": "2020-10-29T14:22:03.242483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are your best parameters:\n",
      "{'alpha': 15, 'solver': 'sparse_cg'}\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_searching(X_train, Y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final predictions and the submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:30:07.636903Z",
     "start_time": "2020-10-29T14:30:07.617425Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_test_dataset(path):\n",
    "    \"\"\"\n",
    "    Simple function to prepare the test dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.drop(columns = COLS_TO_DROP)\n",
    "    data.rename(columns={col: clean_column(col) for col in data.columns.values}, \n",
    "                 inplace=True)\n",
    "    data = encode_variables(data, cols_to_encode)\n",
    "    data = replacing_nans(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_final_predictions(path, titulo):\n",
    "    \"\"\"\n",
    "    This function uses the best model as selected above and makes predictions \n",
    "    for the test data. \n",
    "    \"\"\"\n",
    "    X_test = clean_test_dataset(path)\n",
    "    X_test_prepared = full_pipeline.transform(X_test.iloc[:,:-1]) \n",
    "    #the last column is an \"ID\" column we don't need\n",
    "    final_predictions = final_model.predict(X_test_prepared)\n",
    "    predictions_exp = np.exp(final_predictions) #we predicted for ln(y)\n",
    "    submissions = pd.DataFrame({'id': [e for e in range(1,1204)], 'SalePrice': predictions_exp})\n",
    "    submissions.to_csv(titulo) #save the model\n",
    "    print(\"Your submissions are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:30:28.368514Z",
     "start_time": "2020-10-29T14:30:28.270428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submissions are ready!\n"
     ]
    }
   ],
   "source": [
    "TITULO_MODELO = \"30_octubre.csv\"\n",
    "make_final_predictions('casas_prueba.csv', TITULO_MODELO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T14:26:15.064262Z",
     "start_time": "2020-10-29T14:26:15.061242Z"
    }
   },
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#joblib.dump(final_model, \"29_octubre.pkl\")\n",
    "#y luego lo cargas: \n",
    "#modelo_cargado = joblib.load(\"modelo_uno.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
